{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "python3-asyncpg is already the newest version (0.29.0-1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "python3-pandas is already the newest version (2.1.4+dfsg-7).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.21.4-1ubuntu4.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
      "File ‘postgresql-42.2.5.jar’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y python3-asyncpg\n",
    "!apt-get install -y python3-pandas\n",
    "\n",
    "!apt-get install -y wget\n",
    "!wget -nc https://jdbc.postgresql.org/download/postgresql-42.2.5.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+--------------------+-------------+--------------------+--------------------+--------------------+-------------+----------------+-------------------+--------------------+-----+------------+--------------------+\n",
      "|   id|               date|      deadline_date|               title|     category|         description|               phase|               place|awarded_value|awarded_currency|       awarded_date|      suppliers_name|count|offers_count|        request_json|\n",
      "+-----+-------------------+-------------------+--------------------+-------------+--------------------+--------------------+--------------------+-------------+----------------+-------------------+--------------------+-----+------------+--------------------+\n",
      "|36077|2021-08-01 00:00:00|2021-08-12 00:00:00|\"Régészeti feltár...|constructions|Vállalkozási kere...|E60 - Szerződéskö...|     HU MAGYARORSZÁG|       2.99E8|             HUF|2021-11-17 00:00:00|Salisbury Régésze...|    1|           2|{\"page_count\": 36...|\n",
      "|36073|2021-09-26 00:00:00|2021-10-07 00:00:00|Cserepes és Szélm...|constructions|1. részajánlat: K...|E60 - Szerződéskö...|HU322 Jász-Nagyku...|   5.614704E7|             HUF|2021-11-17 00:00:00|D-PROFIL Építő, K...|    2|           3|{\"page_count\": 36...|\n",
      "|36067|2021-01-17 00:00:00|2021-01-28 00:00:00|Bölcsőde építése ...|constructions|Dömösi Szivárvány...|E60 - Szerződéskö...|HU212 Komárom-Esz...|  5.1809732E7|             HUF|2021-11-17 00:00:00|Mészáros Épületgé...|    1|           3|{\"page_count\": 36...|\n",
      "|36060|2021-01-28 00:00:00|2021-03-03 00:00:00|Építési beruházás...|constructions|Ajánlatkérő az EF...|E60 - Szerződéskö...|         HU332 Békés| 2.04326253E8|             HUF|2021-11-17 00:00:00|BÓLEM Építőipari ...|    6|           2|{\"page_count\": 36...|\n",
      "|36059|2021-10-21 00:00:00|2021-11-03 00:00:00|Felszíni vízrende...|constructions|A belterület véde...|E60 - Szerződéskö...|         HU211 Fejér| 1.38496654E8|             HUF|2021-11-17 00:00:00|Dominik és Szép K...|    1|           3|{\"page_count\": 36...|\n",
      "+-----+-------------------+-------------------+--------------------+-------------+--------------------+--------------------+--------------------+-------------+----------------+-------------------+--------------------+-----+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    161\u001b[39m             continue_fetching = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    156\u001b[39m         page += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo data fetched or an error occurred.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_json, to_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, MapType, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\", \"admin\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\", \"admin\"),\n",
    "    \"database\": os.getenv(\"POSTGRES_DB\", \"tender\"),\n",
    "    \"host\": os.getenv(\"POSTGRES_HOST\", \"postgre\"),\n",
    "    \"port\": int(os.getenv(\"POSTGRES_PORT\", 5432))\n",
    "}\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "jar_path = current_dir / \"postgresql-42.2.5.jar\"\n",
    "\n",
    "if not jar_path.exists():\n",
    "    print(f\"PostgreSQL JDBC Driver not found at {jar_path}. Please download it from https://jdbc.postgresql.org/download.html\")\n",
    "    raise FileNotFoundError(f\"PostgreSQL JDBC Driver not found: {jar_path}\")\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"REST_API_with_PySpark_DF\") \\\n",
    "                    .config(\"spark.jars\", jar_path) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"deadline_date\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"phase\", StringType(), True),\n",
    "    StructField(\"place\", StringType(), True),\n",
    "    StructField(\"awarded_value\", StringType(), True),\n",
    "    StructField(\"awarded_currency\", StringType(), True),\n",
    "    StructField(\"purchaser_name\", StringType(), True),\n",
    "    StructField(\"awarded\", ArrayType(StructType([\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"suppliers\", ArrayType(StructType([ \n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"id\", LongType(), True),\n",
    "            StructField(\"slug\", StringType(), True)\n",
    "        ])), True),\n",
    "        StructField(\"count\", LongType(), True),\n",
    "        StructField(\"offers_count\", ArrayType(LongType()), True),\n",
    "        StructField(\"suppliers_name\", StringType(), True),\n",
    "        StructField(\"value\", LongType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "def get_tenders(page=1):\n",
    "    url = f\"https://tenders.guru/api/hu/tenders?page={page}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    try:\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_df(data, json_data, schema):\n",
    "    \"\"\"\n",
    "    Create a DataFrame from the fetched data and select relevant columns.\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    # Flatten nested fields\n",
    "    df = df.withColumn(\"awarded_date\", F.col(\"awarded\").getItem(0).getField(\"date\")) \\\n",
    "           .withColumn(\"suppliers_name\", F.col(\"awarded\").getItem(0).getField(\"suppliers_name\")) \\\n",
    "           .withColumn(\"count\", F.col(\"awarded\").getItem(0).getField(\"count\")) \\\n",
    "           .withColumn(\"offers_count\", F.col(\"awarded\").getItem(0).getField(\"offers_count\").getItem(0)) \\\n",
    "           .withColumn(\"request\", F.lit(json.dumps(json_data)))\n",
    "\n",
    "    # Cast\n",
    "    df = df.withColumn(\"id\", F.col(\"id\").cast(IntegerType())) \\\n",
    "           .withColumn(\"date\", F.col(\"date\").cast(TimestampType())) \\\n",
    "           .withColumn(\"deadline_date\", F.col(\"deadline_date\").cast(TimestampType())) \\\n",
    "           .withColumn(\"title\", F.col(\"title\").cast(StringType())) \\\n",
    "           .withColumn(\"category\", F.col(\"category\").cast(StringType())) \\\n",
    "           .withColumn(\"description\", F.col(\"description\").cast(StringType())) \\\n",
    "           .withColumn(\"phase\", F.col(\"phase\").cast(StringType())) \\\n",
    "           .withColumn(\"place\", F.col(\"place\").cast(StringType())) \\\n",
    "           .withColumn(\"awarded_value\", F.col(\"awarded_value\").cast(DoubleType())) \\\n",
    "           .withColumn(\"awarded_currency\", F.col(\"awarded_currency\").cast(StringType())) \\\n",
    "           .withColumn(\"awarded_date\", F.col(\"awarded_date\").cast(TimestampType())) \\\n",
    "           .withColumn(\"suppliers_name\", F.col(\"suppliers_name\").cast(StringType())) \\\n",
    "           .withColumn(\"count\", F.col(\"count\").cast(IntegerType())) \\\n",
    "           .withColumn(\"offers_count\", F.col(\"offers_count\").cast(IntegerType())) \\\n",
    "           .withColumn(\"request_json\", F.col(\"request\").cast(StringType())) # original request json as str\n",
    "\n",
    "    selected_columns = [\n",
    "        \"id\", \n",
    "        \"date\", \n",
    "        \"deadline_date\", \n",
    "        \"title\", \n",
    "        \"category\", \n",
    "        \"description\", \n",
    "        \"phase\", \n",
    "        \"place\", \n",
    "        \"awarded_value\", \n",
    "        \"awarded_currency\",\n",
    "        \"awarded_date\", \n",
    "        \"suppliers_name\",\n",
    "        \"count\", \n",
    "        \"offers_count\",\n",
    "        \"request_json\"\n",
    "    ]\n",
    "    \n",
    "    return df.select(*selected_columns)\n",
    "\n",
    "def insert_to_postgres(df, table_name):\n",
    "    \"\"\"\n",
    "    Insert the DataFrame into PostgreSQL using JDBC.\n",
    "    https://stackoverflow.com/questions/77279712/how-to-insert-json-string-from-spark-into-column-of-type-jsonb-in-postgres\n",
    "    \"\"\"\n",
    "    df.write \\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"url\", f\"jdbc:postgresql://{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\") \\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "      .option(\"dbtable\", \"tenders\") \\\n",
    "      .option(\"user\", DB_CONFIG[\"user\"]) \\\n",
    "      .option(\"password\", DB_CONFIG[\"password\"]) \\\n",
    "      .option(\"stringtype\", \"unspecified\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "def main():\n",
    "    page = 1\n",
    "    continue_fetching = True\n",
    "\n",
    "    while continue_fetching:\n",
    "        json_data = get_tenders(page)\n",
    "\n",
    "        if json_data is not None:\n",
    "            data = json_data[\"data\"]\n",
    "            total_pages = json_data['page_count']\n",
    "\n",
    "            df = create_df(data, json_data, schema)\n",
    "\n",
    "            df.show(5)\n",
    "\n",
    "            insert_to_postgres(df, \"tenders\")\n",
    "\n",
    "            if page >= total_pages:\n",
    "                continue_fetching = False\n",
    "            else:\n",
    "                page += 1\n",
    "\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(\"No data fetched or an error occurred.\")\n",
    "            continue_fetching = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
